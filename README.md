# ðŸ§  Fine-Tuning LLMs with PEFT

PyTorch | Hugging Face | PEFT

A comprehensive repo for parameter-efficient fine-tuning (PEFT) of large language models using lightweight methods like LoRA, Adapters, and Prompt Tuning. Designed for minimal compute, high efficiency, and fast iteration.

ðŸš€ Key Features

ðŸ”§ PEFT Techniques
LoRA (Low-Rank Adaptation), Adapter Layers, Prefix Tuning, Prompt Tuning

ðŸ§¬ Model Support
âœ… GPT | Whisper | LLaMA | BERT | DistilBERT | TinyBERT | MobileBERT

ðŸ“‚ Tasks Covered
Sentiment Analysis
Named Entity Recognition (NER)
Text Classification
ðŸ“ˆ Efficiency First
Achieve 90â€“95% of full fine-tuned model performance
using <5% of total parameters

ðŸ§ª Reproducibility
Built with ðŸ¤— Hugging Face transformers, peft, accelerate
ðŸ“š Techniques Covered

Method	Use Case	Example Models
LoRA	Low-rank decomposition	LLaMA-2, BERT variants
Adapters	Lightweight fine-tuning	DistilBERT, TinyBERT
Prefix Tuning	Prompt optimization	GPT-style architectures

âš¡ Quick Start

# 1. Install dependencies
pip install torch transformers peft accelerate datasets

# 2. Clone the repo
git clone https://github.com/pjmreddy/FinetuningLLMs
cd FinetuningLLMs

# 3. Run example script
python finetune_sentiment.py --model_name bert-base-uncased --method lora
ðŸ§  Example Notebooks

âœ… Finetuning_LLaMA2.ipynb
âœ… NER_using_DistilBERT.ipynb
âœ… Sentiment_Classification_BERT.ipynb
âœ… Comparing_DistilBERT_TinyBERT_MobileBERT.ipynb
ðŸ“ Trained on

LONI HPC Clusters â€“ High-performance compute optimized for large model training & experimentation.

Let me know if you want to generate badges, add model performance tables, or integrate Colab links!
